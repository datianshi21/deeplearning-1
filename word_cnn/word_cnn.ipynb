{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "# for python 2.x\n",
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"model_dir\", \"./model_dir\", \"Base directory for the model.\")\n",
    "flags.DEFINE_float(\"dropout_rate\", 0.25, \"Drop out rate\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "flags.DEFINE_integer(\"embedding_size\", 128, \"embedding size\")\n",
    "flags.DEFINE_integer(\"num_filters\", 100, \"number of filters\")\n",
    "flags.DEFINE_integer(\"num_classes\", 14, \"number of classes\")\n",
    "flags.DEFINE_integer(\"shuffle_buffer_size\", 1000000, \"dataset shuffle buffer size\")\n",
    "flags.DEFINE_integer(\"sentence_max_len\", 100, \"max length of sentences\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"number of instances in a batch\")\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 5000, \"Save checkpoints every this many steps\")\n",
    "flags.DEFINE_integer(\"train_steps\", 10000,\n",
    "                     \"Number of (global) training steps to perform\")\n",
    "flags.DEFINE_string(\"data_dir\", \"./dbpedia_csv\", \"Directory containing the dataset\")\n",
    "flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated list of number of window size in each filter\")\n",
    "flags.DEFINE_string(\"pad_word\", \"<pad>\", \"used for pad sentence\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_line(line, vocab):\n",
    "  def get_content(record):\n",
    "    fields = record.decode().split(\",\")\n",
    "    if len(fields) < 3:\n",
    "      raise ValueError(\"invalid record %s\" % record)\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\'\\`]\", \" \", fields[2])\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\`\", \"\\'\", text)\n",
    "    text = text.strip().lower()\n",
    "    tokens = text.split()\n",
    "    tokens = [w.strip(\"'\") for w in tokens if len(w.strip(\"'\")) > 0]\n",
    "    n = len(tokens)  # type: int\n",
    "    if n > FLAGS.sentence_max_len:\n",
    "      tokens = tokens[:FLAGS.sentence_max_len]\n",
    "    if n < FLAGS.sentence_max_len:\n",
    "      tokens += [FLAGS.pad_word] * (FLAGS.sentence_max_len - n)\n",
    "    return [tokens, np.int32(fields[0])]\n",
    "\n",
    "  result = tf.py_func(get_content, [line], [tf.string, tf.int32])\n",
    "  #print (result)\n",
    "  result[0].set_shape([FLAGS.sentence_max_len])\n",
    "  result[1].set_shape([])\n",
    "  # Lookup tokens to return their ids\n",
    "  ids = vocab.lookup(result[0])\n",
    "  #print(\"Ids: \", ids)\n",
    "  return {\"sentence\": ids}, result[1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Peeks\n",
    "tokens and fileds[0] data format and exampleï¼š\n",
    "Tokens:  ['chin', 'shunshin', 'or', 'chen', 'shunchen', 'born', 'february', '18', '1924', 'is', 'a', 'taiwanese', 'japanese', 'novelist', 'translator', 'and', 'cultural', 'critic', 'he', 'is', 'best', 'known', 'for', 'his', 'historical', 'fictions', 'and', 'mystery', 'novels', 'including', 'first', 'opium', 'war', 'chinese', 'history', 'ryukyu', 'wind', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(path_csv, path_vocab, shuffle_buffer_size, num_oov_buckets):\n",
    "  \"\"\"Create tf.data Instance from csv file\n",
    "  Args:\n",
    "      path_csv: (string) path containing one example per line\n",
    "      vocab: (tf.lookuptable)\n",
    "  Returns:\n",
    "      dataset: (tf.Dataset) yielding list of ids of tokens and labels for each example\n",
    "  \"\"\"\n",
    "  vocab = tf.contrib.lookup.index_table_from_file(path_vocab, num_oov_buckets=num_oov_buckets)\n",
    "  # Load txt file, one example per line\n",
    "  dataset = tf.data.TextLineDataset(path_csv)\n",
    "  # Convert line into list of tokens, splitting by white space\n",
    "  dataset = dataset.map(lambda line: parse_line(line, vocab))\n",
    "  if shuffle_buffer_size > 0:\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat()\n",
    "  dataset = dataset.batch(FLAGS.batch_size).prefetch(1)\n",
    "  print(dataset.output_types)\n",
    "  print(dataset.output_shapes)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "  sentence = features['sentence']\n",
    "  # Get word embeddings for each token in the sentence\n",
    "  embeddings = tf.get_variable(name=\"embeddings\", dtype=tf.float32,\n",
    "                               shape=[params[\"vocab_size\"], FLAGS.embedding_size])\n",
    "  sentence = tf.nn.embedding_lookup(embeddings, sentence) # shape:(batch, sentence_len, embedding_size)\n",
    "  # add a channel dim, required by the conv2d and max_pooling2d method\n",
    "  sentence = tf.expand_dims(sentence, -1) # shape:(batch, sentence_len/height, embedding_size/width, channels=1)\n",
    "\n",
    "  pooled_outputs = []\n",
    "  for filter_size in params[\"filter_sizes\"]:\n",
    "      conv = tf.layers.conv2d(\n",
    "          sentence,\n",
    "          filters=FLAGS.num_filters,\n",
    "          kernel_size=[filter_size, FLAGS.embedding_size],\n",
    "          strides=(1, 1),\n",
    "          padding=\"VALID\",\n",
    "          activation=tf.nn.relu)\n",
    "      pool = tf.layers.max_pooling2d(\n",
    "          conv,\n",
    "          pool_size=[FLAGS.sentence_max_len - filter_size + 1, 1],\n",
    "          strides=(1, 1),\n",
    "          padding=\"VALID\")\n",
    "      pooled_outputs.append(pool)\n",
    "  h_pool = tf.concat(pooled_outputs, 3) # shape: (batch, 1, len(filter_size) * embedding_size, 1)\n",
    "  #h_pool_flat = tf.reshape(h_pool, [-1, FLAGS.num_filters * len(params[\"filter_sizes\"])])\n",
    "  #shape: (batch, len(filter_size) * embedding_size)\n",
    "  h_pool_flat = tf.reshape(h_pool, [-1, FLAGS.num_filters * 3])\n",
    "  if 'dropout_rate' in params and params['dropout_rate'] > 0.0:\n",
    "    h_pool_flat = tf.layers.dropout(h_pool_flat, params['dropout_rate'], training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "  logits = tf.layers.dense(h_pool_flat, FLAGS.num_classes, activation=None)\n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate=params['learning_rate'])\n",
    "  def _train_op_fn(loss):\n",
    "    return optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "  my_head = tf.contrib.estimator.multi_class_head(n_classes=FLAGS.num_classes)\n",
    "  return my_head.create_estimator_spec(\n",
    "    features=features,\n",
    "    mode=mode,\n",
    "    labels=labels,\n",
    "    logits=logits,\n",
    "    train_op_fn=_train_op_fn\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  # Load the parameters from the dataset, that gives the size etc. into params\n",
    "  json_path = os.path.join(FLAGS.data_dir, 'dataset_params.json')\n",
    "  assert os.path.isfile(json_path), \"No json file found at {}, run build_vocab.py\".format(json_path)\n",
    "  # Loads parameters from json file\n",
    "  with open(json_path) as f:\n",
    "    config = json.load(f)\n",
    "  FLAGS.pad_word = config[\"pad_word\"]\n",
    "  if config[\"train_size\"] < FLAGS.shuffle_buffer_size:\n",
    "    FLAGS.shuffle_buffer_size = config[\"train_size\"]\n",
    "  print(\"shuffle_buffer_size:\", FLAGS.shuffle_buffer_size)\n",
    "\n",
    "  # Get paths for vocabularies and dataset\n",
    "  path_words = os.path.join(FLAGS.data_dir, 'words.txt')\n",
    "  assert os.path.isfile(path_words), \"No vocab file found at {}, run build_vocab.py first\".format(path_words)\n",
    "  #words = tf.contrib.lookup.index_table_from_file(path_words, num_oov_buckets=config[\"num_oov_buckets\"])\n",
    "\n",
    "  path_train = os.path.join(FLAGS.data_dir, 'train.csv')\n",
    "  path_eval = os.path.join(FLAGS.data_dir, 'test.csv')\n",
    "\n",
    "  classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "      'vocab_size': config[\"vocab_size\"],\n",
    "      'filter_sizes': map(int, FLAGS.filter_sizes.split(',')),\n",
    "      'learning_rate': FLAGS.learning_rate,\n",
    "      'dropout_rate': FLAGS.dropout_rate\n",
    "    },\n",
    "    config=tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps=FLAGS.save_checkpoints_steps)\n",
    "  )\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda: input_fn(path_train, path_words, FLAGS.shuffle_buffer_size, config[\"num_oov_buckets\"]),\n",
    "    max_steps=FLAGS.train_steps\n",
    "  )\n",
    "  input_fn_for_eval = lambda: input_fn(path_eval, path_words, 0, config[\"num_oov_buckets\"])\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn=input_fn_for_eval, throttle_secs=300)\n",
    "\n",
    "  print(\"before train and evaluate\")\n",
    "  #tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n",
    "  print(\"after train and evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle_buffer_size: 560000\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181b95d208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "before train and evaluate\n",
      "after train and evaluate\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zuyan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
